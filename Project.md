# ğŸ¤– AIÂ Robot â€“ Voice InteractionÂ Project

## 1Â Â·Â Overview

This repo contains the **minimal, fullyâ€‘local voice pipeline** for my PiCarâ€‘X robot:

```
PiÂ 5 (wakeâ€‘word + VAD)  â”€â”€WAVâ”€â”€â–¶  FastAPIÂ /ask  â”€â”€MP3â”€â”€â–¶  PiÂ 5 loudâ€‘speaker
                       (LAN < 10Â ms)
        STT  Whisperâ€‘tiny   â‡£      LLM  â€œrobotâ€‘mistralâ€ (Ollama)
                                  â‡¡
                       TTS  Piper (fr_FRâ€‘siwis)
```

* **All computation stays on my network** â€“ no OpenAI cloud calls.
* **Multilingual**: Whisper autoâ€‘detects FRâ€¯/â€¯ENâ€¯/â€¯ES; the answer is voiced in Siwisâ€‘FR for now.
* **Security**: every request needs a `Bearer` API\_TOKEN (+ failâ€‘delay).

---

## 2Â Â·Â File map

| Path                           | Role                                                          |
| ------------------------------ | ------------------------------------------------------------- |
| `sc/server/ask_server.py`                | FastAPI endpoint `/ask` â†’ STTÂ â†’ LLMÂ â†’ TTSÂ â†’ MP3               |
| `src/client/client.py` *(to be finished)* | Runs on the PiÂ 5 â€“ wakeâ€‘word, VAD, POST wav, play mp3         |
| `.env`                         | Stores `API_TOKEN=` so the server never hardâ€‘codes the secret |
| `TTS/fr_FRâ€‘siwisâ€‘medium.onnx`  | Single Piper voice (â‰ˆÂ 60Â MB)                                  |
| `Models/**`      | Ollama personality & system prompt                            |

---

## 3Â Â·Â ask\_server.py responsibilities

1. **Auth**Â â€“ `check_auth()` validates `Authorization: Bearer <TOKEN>` and adds a 2â€‘4Â s delay on bad tokens.
2. **STT**Â â€“ writes the uploaded WAV to a temp file, runs `fasterâ€‘whisper` (tiny INT8, GPU if available).
3. **LLM**Â â€“ calls Ollama (`model: robotâ€‘mistral`) with the user text, gets a reply string.
4. **TTS**Â â€“ pipes the reply into Piper (`fr_FRâ€‘siwisâ€‘medium.onnx`), converts stdout WAV â†’ MP3 using *pydubÂ +Â ffmpeg*.
5. **Response**Â â€“ returns JSON `{ answer, audio }` where `audio` is the MP3 hex string.

> âš Â Dependencies: `python-dotenv fastapi uvicorn faster-whisper[CUDA] piper-tts pydub ffmpeg` + Torch 2.5.1Â cu121 and CTranslate2 GPU.

### QuickÂ start (PCÂ Windows)

```powershell
$Env:API_TOKEN = "<yourâ€‘token>"
pip install -r requirement.txt  # see versions pinned
ollama serve                    # robotâ€‘mistral must be pulled
python ask_server.py            # launches on http://localhost:8000
```

### Test with curl.exe

```powershell
curl.exe -X POST http://localhost:8000/ask ^
 -H "Authorization: Bearer %API_TOKEN%" ^
 -F "file=@test.wav" > reply.json
python utils\play_json.py reply.json  # writes response.mp3 & plays it
```

---

## 4Â Â·Â Client responsibilities (RaspberryÂ Pi)

* **Wakeâ€‘word** with Porcupine: triggers "HeyÂ Mars".
* **VAD capture** (webrtcvad) until 500Â ms silence.
* **POST** to `/ask` with token header.
* **Play** the returned MP3 (`mpg123 -`).
* **Loop** back to listening (disable wakeâ€‘word while audio is playing).

Implementation stub is in `client.py`; next tasks:

1. Glue Porcupine + VAD + HTTP.
2. Add LED "listening" feedback.

---

## 5Â Â·Â Current state

### âœ… Working

* ask_server.py boots, loads token from `.env`.
* Ollama & Whisper GPU run; ffmpeg installed.
* LLM communication works correctly with mars-ia-mistral-nemo model.
* TTS with Piper fully operational using Python API.


### â¬œ To finish

| Item                     | Notes                                                 |
| ------------------------ | ----------------------------------------------------- |
| **Client loop on Pi**    | record, send, play â€“ needs final glue.                |
| **Continuous wake-word** | disable during playback to avoid self-trigger.        |
| **Voice variety**        | add EN/ES Piper voices + languageâ†’voice map.          |
| **Robo-commands**        | later: Rhino or LLM function-calling to drive motors. |


### Known Issues

1. **Piper-TTS Dependency Conflict** (RÃ©solu):
   ```
   Solution: Utiliser piper-phonemize-fix Ã  la place de piper-phonemize
   Commandes:
   pip uninstall piper-tts
   pip install piper-tts==1.2.0 --no-deps
   pip install piper-phonemize-fix==1.2.1
   ```

2. **Utilisation de Piper TTS**:
   ```python
   from piper import PiperVoice
   import wave
   
   # Charger la voix
   voice = PiperVoice.load("chemin/vers/voix.onnx")
   
   # SynthÃ©tiser un Ã©chantillon audio
   with wave.open("output.wav", "wb") as wav_file:
       voice.synthesize("Texte Ã  synthÃ©tiser", wav_file)
   ```


### ğŸ Known issues

* `pydub` warns if ffmpeg isnâ€™t in PATH â€“ solved by installing *GyanÂ FFmpeg*.
* `API_TOKEN` must be in env **before** launching the server, or .env + `pythonâ€‘dotenv`.
* Upload must be a **wav** file; other formats will need an ffmpeg decode step.

---

## 6Â Â·Â NextÂ steps

1. Finish `client.py` and test the full LAN roundâ€‘trip.
2. Benchmark latency & tokens/s, tweak Whisper size (tiny â†’ base) if accuracy too low.
3. Introduce Piper EN/ES or switch to Orca if we need faster TTS.
4. Add command intents (Rhino) or Ollama functionâ€‘calling for robot motion.

Happy hacking! ğŸš€
