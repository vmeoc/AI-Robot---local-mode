# ðŸ¤– AI Robot â€“ Voice Interaction Project

## 1 Â· Overview

This repo contains the **minimal, fullyâ€‘local voice pipeline** for my PiCarâ€‘X robot:

```
Pi 5 (wakeâ€‘word + VAD)  â”€â”€WAVâ”€â”€â–¶  FastAPI /ask  â”€â”€MP3â”€â”€â–¶  Pi 5 loudâ€‘speaker
                       (LAN < 10 ms)
        STT  Whisperâ€‘base   â‡£      LLM  "robotâ€‘mistral" (Ollama)
                                  â‡¡
                       TTS  Piper (fr_FRâ€‘siwis)
```

* **All computation stays on my network** â€“ no OpenAI cloud calls.
* **Multilingual**: Whisper autoâ€‘detects FR / EN / ES; the answer is voiced in Siwisâ€‘FR for now.
* **Security**: every request needs a `Bearer` API\_TOKEN (+ failâ€‘delay).

---

## 2 Â· File map

| Path                           | Role                                                          |
| ------------------------------ | ------------------------------------------------------------- |
| `src/server/ask_server.py`     | FastAPI endpoint `/ask` â†’ STT â†’ LLM (Text+Actions) â†’ TTS â†’ Returns MP3 & Action List |
| `src/client/client.py`         | Runs on Pi 5 â€“ wakeâ€‘word, VAD, POST wav, plays MP3, executes actions based on LLM output |
| `.env`                         | Stores `API_TOKEN=` so the server never hardâ€‘codes the secret |
| `TTS/fr_FRâ€‘siwisâ€‘medium.onnx`  | Single Piper voice (â‰ˆ 60 MB)                                  |
| `Models/**`                    | Ollama personality & system prompt                            |
| `src/client/AUDIO_IMPROVEMENTS.md` | Documentation des amÃ©liorations audio                     |
| `src/client/preset_actions.py` | Defines robot actions (functions) mapped to string keys used by LLM |

---

## 3 Â· ask\_server.py responsibilities

1. **Auth** â€“ `check_auth()` validates `Authorization: Bearer <TOKEN>` and adds a 2â€‘4 s delay on bad tokens.
2. **STT** â€“ writes the uploaded WAV to a temp file, runs `fasterâ€‘whisper` (base model, float16/int8, GPU if available).
3. **LLM** â€“ calls Ollama (`model: mars-ia-llama3-8B-instruct-q4`) with the user text, gets a reply string.
4. **TTS** â€“ pipes the reply into Piper (`fr_FRâ€‘siwisâ€‘medium.onnx`), converts stdout WAV â†’ MP3 using *pydub + ffmpeg*.
5. **Response** â€“ returns JSON `{ answer, audio }` where `audio` is the MP3 hex string.

> âš  Dependencies: `python-dotenv fastapi uvicorn faster-whisper[CUDA] piper-tts pydub ffmpeg` + Torch 2.5.1 cu121 and CTranslate2 GPU.

### Quick start (PC Windows)

```powershell
$Env:API_TOKEN = "<yourâ€‘token>"
pip install -r requirement.txt  # see versions pinned
ollama serve                    # mars-ia-llama3-8B-instruct-q4 must be pulled
python ask_server.py            # launches on http://localhost:8000
```

### Test with curl.exe

```powershell
curl.exe -X POST http://localhost:8000/ask ^
 -H "Authorization: Bearer %API_TOKEN%" ^
 -F "file=@test.wav" > reply.json
python utils\play_json.py reply.json  # writes response.mp3 & plays it
```

---

## 4 Â· Client responsibilities (Raspberry Pi)

* **Audio Processing** â€“ High-pass filter, normalization, SNR calculation
* **Smart VAD** â€“ Multi-criteria detection with confidence scoring
* **Auto-calibration** â€“ Adapts to ambient noise with percentile-based thresholds
* **Audio Input** â€“ Uses ALSA's default capture device (override with `--input-device`)
* **POST** to `/ask` with token header and normalized audio
* **Play** the returned MP3 via robot_hat Music API
* **Statistics** â€“ Real-time SNR monitoring and session stats

---

## 5 Â· Current state

### âœ… Working

* ask_server.py boots, loads token from `.env`.
* Ollama & Whisper GPU run; ffmpeg installed.
* LLM communication works correctly with mars-ia-llama3-8B-instruct-q4 model.
* TTS with Piper fully operational using Python API.
* **NEW**: Client audio capture optimized with filtering and normalization
* **NEW**: Whisper base model for better STT accuracy
* **NEW**: Multi-criteria speech detection with SNR monitoring
* **NEW**: Automatic noise floor calibration
* **NEW**: Real-time audio quality indicators

### â¬œ To finish

| Item                     | Notes                                                 |
| ------------------------ | ----------------------------------------------------- |
| **Wake-word integration** | Re-enable Porcupine for hands-free activation       |
| **Voice variety**        | add EN/ES Piper voices + languageâ†’voice map.          |


### ðŸŽ¤ Audio Quality Improvements

1. **Signal Processing**:
   - High-pass filter (80Hz cutoff) removes low-frequency noise
   - Audio normalization ensures consistent levels
   - Real-time SNR calculation for quality monitoring

2. **Speech Detection**:
   - Multi-criteria scoring: audio level + VAD + variation + SNR
   - Adaptive thresholds based on noise percentiles
   - Pre-buffer captures speech onset
   - Automatic recalibration on high false-positive rate

3. **Whisper Optimization**:
   - Upgraded from `tiny` to `base` model
   - VAD filter enabled with tuned parameters
   - Beam size increased to 5
   - Initial prompt helps with French recognition

4. **Robot Actions and Synchronization**:
      - The LLM, guided by an updated system prompt, now returns a JSON object containing both the textual response (`answer_text`) and a list of desired robot actions (`actions_list`).
      - `ask_server.py` parses this JSON. The `answer_text` is sent to TTS, and the `actions_list` is forwarded to the client alongside the audio.
      - `client.py` on the PiCar-X receives both the audio and the `actions_list`.
      - It plays the audio and then iterates through the `actions_list`, executing each action by calling corresponding functions defined in `src/client/preset_actions.py`.
      - This allows for synchronized speech and movement, making the robot's interactions more expressive and engaging.
      - Action names in the LLM prompt are carefully matched with function keys in `preset_actions.py` to ensure correct execution.

### Known Issues

1. **Piper-TTS Dependency Conflict** (RÃ©solu):
   ```
   Solution: Utiliser piper-phonemize-fix Ã  la place de piper-phonemize
   Commandes:
   pip uninstall piper-tts
   pip install piper-tts==1.2.0 --no-deps
   pip install piper-phonemize-fix==1.2.1
   ```

2. **Scipy Installation on Raspberry Pi**:
   ```bash
   # Install system dependencies first
   sudo apt-get install libatlas-base-dev gfortran
   # Then install scipy
   pip install scipy==1.10.1
   ```

### ðŸž Known issues

* `pydub` warns if ffmpeg isn't in PATH â€“ solved by installing *Gyan FFmpeg*.
* `API_TOKEN` must be in env **before** launching the server, or .env + `pythonâ€‘dotenv`.
* Upload must be a **wav** file; other formats will need an ffmpeg decode step.

---

## 6 Â· Next steps

1. **Hardware**: Test with USB microphone for better audio quality (recommended: Blue Yeti Nano, Samson Go Mic)
2. **Local STT**: Consider Vosk for on-device transcription to reduce latency
3. **Wake Word**: Re-integrate Porcupine with improved audio pipeline
4. **Enhanced Voice Commands**: Further refine robot motion capabilities, potentially exploring more complex sequences or context-aware actions beyond the current pre-defined list.
5. ** add english support**
6. ** browse web**

### Usage Tips

* Monitor SNR in real-time - aim for > 10 dB
* Speak 20-30cm from microphone
* Run `update_audio_system.sh` on Pi for optimal setup
* Check `AUDIO_IMPROVEMENTS.md` for detailed documentation

Happy hacking! ðŸš€
