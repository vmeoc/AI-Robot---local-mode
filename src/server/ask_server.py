# ask_server.py  â€“ pipeline STT â†’ LLM â†’ TTS avec une seule voix Piper
import os
import io
import time
import random
import tempfile
import subprocess
import requests
import wave

from dotenv import load_dotenv, find_dotenv
from fastapi import FastAPI, UploadFile, File, Header, HTTPException
from faster_whisper import WhisperModel
import pydub
import json

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  CONFIG  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv(find_dotenv())  # charge un Ã©ventuel fichier .env
API_TOKEN  = os.getenv("API_TOKEN", "")
print("[INFO] API_TOKEN chargÃ© :", API_TOKEN[:8] if API_TOKEN else "<vide>")

OLLAMA_URL = "http://127.0.0.1:11434/api/chat"       # Ollama Â« robot-mistral Â»
VOICE_PATH = r"C:\Users\vince\Documents\VS Code\Dev\AI Robot - local mode\TTS\fr_FR-siwis-medium.onnx"
PIPER_TTS_EXE = r"C:\Users\vince\Documents\VS Code\Dev\AI Robot - local mode\.venv\Scripts\piper-tts.exe"
LLM= "mars-ia-llama3-8B-instruct-q4" #mars-ia-llama3-8B-instruct-q4 ou gemma3:1b
SYSTEM_PROMPT = """Tu es un robot serviable et amusant avec des roues, conÃ§u pour interagir avec des enfants.
RÃ©ponds TOUJOURS en fournissant un objet JSON valide. Cet objet JSON DOIT contenir deux clÃ©s :
1. \"answer_text\": une chaÃ®ne de caractÃ¨res contenant la rÃ©ponse textuelle que tu donneras Ã  voix haute. Cette rÃ©ponse doit Ãªtre courte, engageante, et adaptÃ©e aux enfants. N'utilise pas d'onomatopÃ©es ou de symboles comme '*' dans ce texte.
2. "actions_list": une liste de chaÃ®nes de caractÃ¨res. Chaque chaÃ®ne est le nom d'une action que le robot doit effectuer. Les actions possibles sont EXACTEMENT : "shake head", "nod", "wave hands", "resist", "act cute", "rub hands", "think", "twist body", "celebrate", "depressed", "honking", "start engine". Choisis parmi cette liste. Si aucune action n'est appropriÃ©e, retourne une liste vide [].

Exemple de format de rÃ©ponse attendu :
{
  \"answer_text\": \"Bonjour les amis ! PrÃªts Ã  jouer ?\",
  "actions_list": ["wave hands"]
}
Ou, si aucune action n'est nÃ©cessaire :
{
  \"answer_text\": \"Je suis un robot trÃ¨s intelligent.\",
  \"actions_list\": []
}
Assure-toi que ta rÃ©ponse est UNIQUEMENT cet objet JSON, sans texte avant ou aprÃ¨s."""

# Conversation history
conversation_history = []
MAX_HISTORY_TURNS = 5  # Number of user/assistant pairs to keep
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â¶  SÃ©curitÃ© â€“ fail-delay
def check_auth(hdr: str | None):
    if hdr is None or not hdr.startswith("Bearer "):
        _delay_fail()
    token = hdr.split(None, 1)[1]
    if token != API_TOKEN:
        _delay_fail()

def _delay_fail():
    time.sleep(random.uniform(2, 4))
    raise HTTPException(status_code=401, detail="unauthorized")

# â·  STT multilingue (Whisper base, GPU si dispo)
# Utilisation du modÃ¨le "base" pour une meilleure prÃ©cision
# compute_type="float16" pour plus de prÃ©cision si GPU disponible, sinon "int8"
try:
    # Essayer d'utiliser float16 si GPU disponible
    stt = WhisperModel("small", compute_type="float16")
    print("[INFO] Whisper small chargÃ© avec compute_type=float16 (GPU)")
except Exception:
    # Fallback sur int8 si pas de GPU
    stt = WhisperModel("small", compute_type="int8")
    print("[INFO] Whisper small chargÃ© avec compute_type=int8 (CPU)")

# â¸  Appel Ollama
def llama(prompt: str) -> tuple[str, list]:
    global conversation_history # Declare intent to modify global variable

    # Prepare messages for Ollama, including history
    messages_payload = [{"role": "system", "content": SYSTEM_PROMPT}]
    messages_payload.extend(conversation_history)
    messages_payload.append({"role": "user", "content": prompt})

    try:
        rsp = requests.post(
            OLLAMA_URL,
            json={
                "model": LLM,
                "messages": messages_payload,
                "stream": False,
                "format": "json" # Request JSON format from Ollama if supported
            },
            timeout=60,
        )
        rsp.raise_for_status()  # LÃ¨ve une exception pour les erreurs HTTP (4xx ou 5xx)
        
        response_json_ollama = rsp.json()
        
        if "message" in response_json_ollama and isinstance(response_json_ollama["message"], dict) and "content" in response_json_ollama["message"]:
            assistant_response_str = response_json_ollama["message"]["content"]
            
            final_answer_text = "Je ne sais pas quoi rÃ©pondre."
            final_actions_list = []
            try:
                parsed_llm_output = json.loads(assistant_response_str)
                final_answer_text = parsed_llm_output.get("answer_text", "Pardon, je n'ai pas rÃ©ussi Ã  formuler une rÃ©ponse claire.")
                final_actions_list = parsed_llm_output.get("actions_list", [])
                if not isinstance(final_actions_list, list):
                    print(f"[LLAMA WARNING] 'actions_list' from LLM was not a list: {final_actions_list}. Defaulting to empty list.")
                    final_actions_list = []
            except json.JSONDecodeError:
                print(f"[LLAMA ERROR] Failed to parse LLM JSON response: {assistant_response_str}")
                final_answer_text = assistant_response_str # Fallback to using the raw response as text
                final_actions_list = []
            except AttributeError: # Handles if parsed_llm_output is not a dict
                print(f"[LLAMA ERROR] LLM response was valid JSON but not a dictionary: {assistant_response_str}")
                final_answer_text = str(parsed_llm_output) if 'parsed_llm_output' in locals() else assistant_response_str
                final_actions_list = []

            # Add current exchange to history (using the textual part for clarity)
            conversation_history.append({"role": "user", "content": prompt})
            conversation_history.append({"role": "assistant", "content": final_answer_text})
            
            # Prune history if it's too long
            if len(conversation_history) > MAX_HISTORY_TURNS * 2:
                conversation_history = conversation_history[-(MAX_HISTORY_TURNS * 2):]
                
            return final_answer_text, final_actions_list
        elif "error" in response_json_ollama:
            error_message = response_json_ollama["error"]
            print(f"[LLAMA ERROR] Ollama a retournÃ© une erreur: {error_message}")
            raise HTTPException(status_code=502, detail=f"Ollama API error: {error_message}")
        else:
            print(f"[LLAMA ERROR] Structure de rÃ©ponse inattendue d'Ollama: {response_json_ollama}")
            raise HTTPException(status_code=502, detail=f"Unexpected response structure from Ollama: {response_json_ollama}")
            
    except requests.exceptions.RequestException as e:
        print(f"[LLAMA ERROR] La requÃªte vers Ollama a Ã©chouÃ©: {e}")
        raise HTTPException(status_code=503, detail=f"Service unavailable: Could not connect to Ollama: {e}")
    except KeyError as e:
        response_text = rsp.text if 'rsp' in locals() and hasattr(rsp, 'text') else 'Response object or text not available'
        print(f"[LLAMA ERROR] Erreur lors du parsing de la rÃ©ponse d'Ollama (KeyError: {e}). RÃ©ponse: {response_text}")
        raise HTTPException(status_code=502, detail=f"Error parsing Ollama response: KeyError {e}")
    except Exception as e:
        response_text = rsp.text if 'rsp' in locals() and hasattr(rsp, 'text') else 'Response object or text not available'
        print(f"[LLAMA ERROR] Erreur inattendue dans la fonction llama: {e}. RÃ©ponse: {response_text}")
        raise HTTPException(status_code=500, detail=f"Unexpected error in Llama call: {e}")

# â¹  TTS â€“ unique voix FR (fonctionne mÃªme pour EN/ES)
def synthesize(text: str) -> bytes:
    from piper import PiperVoice
    import wave
    
    # CrÃ©er un buffer en mÃ©moire pour le WAV
    wav_io = io.BytesIO()
    
    # Utiliser wave pour crÃ©er un fichier WAV valide
    with wave.open(wav_io, 'wb') as wav_file:
        voice = PiperVoice.load(VOICE_PATH)
        voice.synthesize(text, wav_file)
    
    # Convertir le WAV en MP3
    wav_io.seek(0)
    mp3 = (
        pydub.AudioSegment.from_wav(wav_io)
        .export(format="mp3")
        .read()
    )
    return mp3

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  API FastAPI  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI()

@app.post("/ask")
async def ask(
    file: UploadFile = File(...),
    authorization: str | None = Header(None),
):
    check_auth(authorization)

    # 1) Sauvegarde des fichiers entrants pour analyse
    os.makedirs("received_audio", exist_ok=True)
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    save_path = f"received_audio/audio_{timestamp}.wav"
    
    audio_bytes = await file.read()
    with open(save_path, "wb") as f:
        f.write(audio_bytes)
    
    # CrÃ©e aussi un fichier temporaire pour Whisper
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp.write(audio_bytes)
        tmp_path = tmp.name

    try:
        # 2) Transcription avec paramÃ¨tres optimisÃ©s
        stt_start = time.time()
        segments, info = stt.transcribe(
            tmp_path,
            beam_size=5,  # AugmentÃ© pour plus de prÃ©cision
            language="fr",  # Forcer le franÃ§ais
            vad_filter=True,  # Activer le VAD interne
            vad_parameters=dict(
                threshold=0.5,  # Seuil de dÃ©tection de parole
                min_speech_duration_ms=250,  # DurÃ©e minimale de parole
                max_speech_duration_s=float('inf'),  # Pas de limite max
                min_silence_duration_ms=1000,  # Silence minimum entre segments
                speech_pad_ms=400  # Padding autour de la parole dÃ©tectÃ©e
            ),
            word_timestamps=False,  # Pas besoin des timestamps par mot
            condition_on_previous_text=True,  # Meilleure cohÃ©rence
            compression_ratio_threshold=2.4,  # Seuil de compression
            log_prob_threshold=-1.0,  # Seuil de probabilitÃ© log
            no_speech_threshold=0.6,  # Seuil de non-parole
            temperature=0.0,  # Pas de sampling alÃ©atoire
            initial_prompt="Ceci est une conversation en franÃ§ais.",  # Aide le modÃ¨le
        )
        stt_duration = time.time() - stt_start
        
        # Afficher plus d'informations sur la transcription
        print(f"[STT TIME] {stt_duration:.2f}s")
        print(f"[STT INFO] Langue dÃ©tectÃ©e: {info.language}, ProbabilitÃ©: {info.language_probability:.2f}")
    finally:
        os.unlink(tmp_path)  # supprime le WAV temporaire

    # Assembler le texte avec gestion des segments vides
    text_segments = []
    for segment in segments:
        if segment.text.strip():  # Ignorer les segments vides
            text_segments.append(segment.text.strip())
    
    text = " ".join(text_segments)
    
    # VÃ©rifier si on a bien capturÃ© du texte
    if not text:
        print("[STT] Aucune parole dÃ©tectÃ©e dans l'audio")
        return {"answer": "Je n'ai pas compris. Pouvez-vous rÃ©pÃ©ter ?", "actions": [], "audio": ""}
    
    print(f"[STT {info.language}] {text}")

    # 3) LLM
    llm_start = time.time()
    answer_text, actions_list = llama(text)
    llm_duration = time.time() - llm_start
    print(f"[LLM TIME] {llm_duration:.2f}s")
    print(f"[LLM] Answer: {answer_text}, Actions: {actions_list}")

    # 4) TTS
    tts_start = time.time()
    mp3_bytes = synthesize(answer_text)
    tts_duration = time.time() - tts_start
    print(f"[TTS TIME] {tts_duration:.2f}s")
    return {"answer": answer_text, "actions": actions_list, "audio": mp3_bytes.hex()}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Endpoint de test  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.get("/health")
async def health_check():
    """Endpoint pour vÃ©rifier que le serveur fonctionne"""
    return {
        "status": "ok",
        "whisper_model": "small",
        "llm_model": "mars-ia-llama3-8B-instruct-q4",
        "tts_voice": "fr_FR-siwis-medium"
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Lancement direct  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import uvicorn
    
    print("=" * 60)
    print("ğŸš€ DÃ©marrage du serveur AI Robot")
    print("=" * 60)
    print(f"ğŸ“ Endpoint: http://0.0.0.0:8000/ask")
    print(f"ğŸ” Token requis: {'Oui' if API_TOKEN else 'Non'}")
    print(f"ğŸ™ï¸  ModÃ¨le STT: Whisper base")
    print(f"ğŸ¤– ModÃ¨le LLM: {LLM}")
    print(f"ğŸ”Š Voix TTS: fr_FR-siwis-medium")
    print("=" * 60)

    uvicorn.run(app, host="0.0.0.0", port=8000)
