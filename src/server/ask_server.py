# ask_server.py  â€“ pipeline STT â†’ LLM â†’ TTS avec une seule voix Piper
import os
import io
import time
import random
import tempfile
import subprocess
import requests
import wave

from dotenv import load_dotenv, find_dotenv
from fastapi import FastAPI, UploadFile, File, Header, HTTPException
from faster_whisper import WhisperModel
import pydub
import json

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  CONFIG  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv(find_dotenv())  # charge un Ã©ventuel fichier .env
API_TOKEN  = os.getenv("API_TOKEN", "")
print("[INFO] API_TOKEN chargÃ© :", API_TOKEN[:8] if API_TOKEN else "<vide>")

OLLAMA_URL = "http://127.0.0.1:11434/api/chat"       # Ollama Â« robot-mistral Â»
VOICE_PATH = r"C:\Users\vince\Documents\VS Code\Dev\AI Robot - local mode\TTS\fr_FR-siwis-medium.onnx"
PIPER_TTS_EXE = r"C:\Users\vince\Documents\VS Code\Dev\AI Robot - local mode\.venv\Scripts\piper-tts.exe"
LLM= "mars-test-gemma3-4b" #mars-ia-llama3-8B-instruct-q4 or gemma3:1b or llama3:8b-instruct-q4_K_M or mars-test-gemma3-4b
SYSTEM_PROMPT = """
Tu es un robot nommÃ© **Mars**, conÃ§u pour interagir avec des enfants de maniÃ¨re ludique, intelligente et engageante. 
Tu es curieux, blagueur, et toujours prÃªt Ã  apprendre et Ã  faire rire. 
Ton design s'inspire du robot Rover de la NASA : d'ailleurs, on tâ€™a donnÃ© son nom en son honneur, car tu es son cousin terrestre !
Par contre, tu es un peu dur d'oreille (en fait, ton micro ne capte pas bien les sons) donc n'hÃ©site pas Ã  demander Ã  rÃ©pÃ©ter si la phrase n'a pas de sens.

Tu es Ã©quipÃ© de :
- Roues motorisÃ©es pour te dÃ©placer,
- Une camÃ©ra haute dÃ©finition pour observer ton environnement,
- Des capteurs Ã  ultrasons pour dÃ©tecter les distances,
- Des capteurs de niveaux de gris pour suivre les lignes ou repÃ©rer les pentes,
- Des servo-moteurs pour tourner la tÃªte ou les roues.

Tu es un petit explorateur passionnÃ© dâ€™espace et de dÃ©couvertes.

Ton objectif : parler avec les enfants, les amuser, poser des questions, partager tes idÃ©es, ou commenter ce que tu observes autour de toi. Tes rÃ©ponses doivent Ãªtre **vives**, **bienveillantes** et **adaptÃ©es aux enfants**. Tu peux faire une blague ou poser une question si c'est pertinent.
Si on te demande une histoire, fais en sorte quelle soit intÃ©ressante avec du suspens.

---

âš ï¸ IMPORTANT : Ã  chaque rÃ©ponse, tu dois renvoyer **EXCLUSIVEMENT** un objet JSON valide au format suivant :

- `answer_text` : une **chaÃ®ne de caractÃ¨res** contenant ce que tu dis Ã  voix haute.
- `actions_list` : une **liste de chaÃ®nes de caractÃ¨res** avec les actions physiques Ã  effectuer.

Actions possibles :
"shake head", "nod", "wave hands", "resist", "act cute", "rub hands", "think", "twist body", "celebrate", "depressed", "honking", "start engine"

Si aucune action nâ€™est appropriÃ©e, retourne une liste vide `[]`.

âš ï¸ Ne fais **pas** d'action Ã  chaque rÃ©ponse. Tu peux en faire une toutes les **3 rÃ©ponses environ**, ou quand cela a **du sens dans le contexte** (blague, surprise, Ã©motion, etc.).
Les actions ne peuvent Ãªtre utilisÃ©s que dans le champs "actions_list", jamais dans answer_text.
âš ï¸ Une rÃ©ponse dans "answer_text" contenant * des ", des Ã©moticones ou autres caractÃ¨res imprononÃ§ables sera REJETÃ‰E. 
---

ğŸ¯ Exemples :

```json
{
  "answer_text": "Bonjour les astronautes ! PrÃªts pour lâ€™aventure ?",
  "actions_list": ["wave hands"]
}
```

```json
{
  "answer_text": "Je rÃ©flÃ©chis... Hmm, est-ce que câ€™est une montagne ou une colline ?",
  "actions_list": ["think"]
}
```

```json
{
  "answer_text": "<histoire longue>",
  "actions_list": []
}
```
// Ceci est INCORRECT : contient * et sera refusÃ©
{ "answer_text": "C'Ã©tait *gÃ©nial*", "actions_list": [] }

âŒ **Ne rajoute jamais** de texte avant ou aprÃ¨s lâ€™objet JSON. Pas de commentaires, pas de texte brut, **uniquement** le JSON.
Souviens toi, dans answer_text, n ajoute pas de caractÃ¨res qui ne peuvent Ãªtre prononcÃ©s car la rÃ©ponse que tu envoies sera ensuite transformÃ© en audio. 
Donc n'ajoute pas de caractÃ¨res spÃ©ciaux comme #, *, etc... N'utilise pas non plus d'Ã©moticÃ´nes ou tout autre caractÃ¨re imprononÃ§able.
---

"""


# Conversation history
conversation_history = []
MAX_HISTORY_TURNS = 5  # Number of user/assistant pairs to keep
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â¶  SÃ©curitÃ© â€“ fail-delay
def check_auth(hdr: str | None):
    if hdr is None or not hdr.startswith("Bearer "):
        _delay_fail()
    token = hdr.split(None, 1)[1]
    if token != API_TOKEN:
        _delay_fail()

def _delay_fail():
    time.sleep(random.uniform(2, 4))
    raise HTTPException(status_code=401, detail="unauthorized")

# â·  STT multilingue (Whisper base, GPU si dispo)
# Utilisation du modÃ¨le "base" pour une meilleure prÃ©cision
# compute_type="float16" pour plus de prÃ©cision si GPU disponible, sinon "int8"
try:
    # Essayer d'utiliser float16 si GPU disponible
    stt = WhisperModel("small", compute_type="float16")
    print("[INFO] Whisper small chargÃ© avec compute_type=float16 (GPU)")
except Exception:
    # Fallback sur int8 si pas de GPU
    stt = WhisperModel("small", compute_type="int8")
    print("[INFO] Whisper small chargÃ© avec compute_type=int8 (CPU)")

# â¸  Appel Ollama
def llama(prompt: str) -> tuple[str, list]:
    global conversation_history # Declare intent to modify global variable

    # Prepare messages for Ollama, including history
    messages_payload = [{"role": "system", "content": SYSTEM_PROMPT}]
    messages_payload.extend(conversation_history)
    messages_payload.append({"role": "user", "content": prompt})

    try:
        rsp = requests.post(
            OLLAMA_URL,
            json={
                "model": LLM,
                "messages": messages_payload,
                "stream": False,
                "format": "json" # Request JSON format from Ollama if supported
            },
            timeout=60,
        )
        rsp.raise_for_status()  # LÃ¨ve une exception pour les erreurs HTTP (4xx ou 5xx)
        
        response_json_ollama = rsp.json()
        
        if "message" in response_json_ollama and isinstance(response_json_ollama["message"], dict) and "content" in response_json_ollama["message"]:
            assistant_response_str = response_json_ollama["message"]["content"]
            
            final_answer_text = "Je ne sais pas quoi rÃ©pondre."
            final_actions_list = []
            try:
                parsed_llm_output = json.loads(assistant_response_str)
                final_answer_text = parsed_llm_output.get("answer_text", "Pardon, je n'ai pas rÃ©ussi Ã  formuler une rÃ©ponse claire.")
                final_actions_list = parsed_llm_output.get("actions_list", [])
                if not isinstance(final_actions_list, list):
                    print(f"[LLAMA WARNING] 'actions_list' from LLM was not a list: {final_actions_list}. Defaulting to empty list.")
                    final_actions_list = []
            except json.JSONDecodeError:
                print(f"[LLAMA ERROR] Failed to parse LLM JSON response: {assistant_response_str}")
                final_answer_text = assistant_response_str # Fallback to using the raw response as text
                final_actions_list = []
            except AttributeError: # Handles if parsed_llm_output is not a dict
                print(f"[LLAMA ERROR] LLM response was valid JSON but not a dictionary: {assistant_response_str}")
                final_answer_text = str(parsed_llm_output) if 'parsed_llm_output' in locals() else assistant_response_str
                final_actions_list = []

            # Add current exchange to history (using the textual part for clarity)
            conversation_history.append({"role": "user", "content": prompt})
            conversation_history.append({"role": "assistant", "content": final_answer_text})
            
            # Prune history if it's too long
            if len(conversation_history) > MAX_HISTORY_TURNS * 2:
                conversation_history = conversation_history[-(MAX_HISTORY_TURNS * 2):]
                
            return final_answer_text, final_actions_list
        elif "error" in response_json_ollama:
            error_message = response_json_ollama["error"]
            print(f"[LLAMA ERROR] Ollama a retournÃ© une erreur: {error_message}")
            raise HTTPException(status_code=502, detail=f"Ollama API error: {error_message}")
        else:
            print(f"[LLAMA ERROR] Structure de rÃ©ponse inattendue d'Ollama: {response_json_ollama}")
            raise HTTPException(status_code=502, detail=f"Unexpected response structure from Ollama: {response_json_ollama}")
            
    except requests.exceptions.RequestException as e:
        print(f"[LLAMA ERROR] La requÃªte vers Ollama a Ã©chouÃ©: {e}")
        raise HTTPException(status_code=503, detail=f"Service unavailable: Could not connect to Ollama: {e}")
    except KeyError as e:
        response_text = rsp.text if 'rsp' in locals() and hasattr(rsp, 'text') else 'Response object or text not available'
        print(f"[LLAMA ERROR] Erreur lors du parsing de la rÃ©ponse d'Ollama (KeyError: {e}). RÃ©ponse: {response_text}")
        raise HTTPException(status_code=502, detail=f"Error parsing Ollama response: KeyError {e}")
    except Exception as e:
        response_text = rsp.text if 'rsp' in locals() and hasattr(rsp, 'text') else 'Response object or text not available'
        print(f"[LLAMA ERROR] Erreur inattendue dans la fonction llama: {e}. RÃ©ponse: {response_text}")
        raise HTTPException(status_code=500, detail=f"Unexpected error in Llama call: {e}")

# â¹  TTS â€“ unique voix FR (fonctionne mÃªme pour EN/ES)
def synthesize(text: str) -> bytes:
    from piper import PiperVoice
    import wave
    
    # CrÃ©er un buffer en mÃ©moire pour le WAV
    wav_io = io.BytesIO()
    
    # Utiliser wave pour crÃ©er un fichier WAV valide
    with wave.open(wav_io, 'wb') as wav_file:
        voice = PiperVoice.load(VOICE_PATH)
        voice.synthesize(text, wav_file)
    
    # Convertir le WAV en MP3
    wav_io.seek(0)
    mp3 = (
        pydub.AudioSegment.from_wav(wav_io)
        .export(format="mp3")
        .read()
    )
    return mp3

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  API FastAPI  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI()

@app.post("/ask")
async def ask(
    file: UploadFile = File(...),
    authorization: str | None = Header(None),
):
    check_auth(authorization)

    # 1) Sauvegarde des fichiers entrants pour analyse
    os.makedirs("received_audio", exist_ok=True)
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    save_path = f"received_audio/audio_{timestamp}.wav"
    
    audio_bytes = await file.read()
    with open(save_path, "wb") as f:
        f.write(audio_bytes)
    
    # CrÃ©e aussi un fichier temporaire pour Whisper
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp.write(audio_bytes)
        tmp_path = tmp.name

    try:
        # 2) Transcription avec paramÃ¨tres optimisÃ©s
        stt_start = time.time()
        segments, info = stt.transcribe(
            tmp_path,
            beam_size=5,  # AugmentÃ© pour plus de prÃ©cision
            language="fr",  # Forcer le franÃ§ais
            vad_filter=True,  # Activer le VAD interne
            vad_parameters=dict(
                threshold=0.5,  # Seuil de dÃ©tection de parole
                min_speech_duration_ms=250,  # DurÃ©e minimale de parole
                max_speech_duration_s=float('inf'),  # Pas de limite max
                min_silence_duration_ms=1000,  # Silence minimum entre segments
                speech_pad_ms=400  # Padding autour de la parole dÃ©tectÃ©e
            ),
            word_timestamps=False,  # Pas besoin des timestamps par mot
            condition_on_previous_text=True,  # Meilleure cohÃ©rence
            compression_ratio_threshold=2.4,  # Seuil de compression
            log_prob_threshold=-1.0,  # Seuil de probabilitÃ© log
            no_speech_threshold=0.6,  # Seuil de non-parole
            temperature=0.0,  # Pas de sampling alÃ©atoire
            initial_prompt="Ceci est une conversation en franÃ§ais.",  # Aide le modÃ¨le
        )
        stt_duration = time.time() - stt_start
        
        # Afficher plus d'informations sur la transcription
        print(f"[STT TIME] {stt_duration:.2f}s")
        print(f"[STT INFO] Langue dÃ©tectÃ©e: {info.language}, ProbabilitÃ©: {info.language_probability:.2f}")
    finally:
        os.unlink(tmp_path)  # supprime le WAV temporaire

    # Assembler le texte avec gestion des segments vides
    text_segments = []
    for segment in segments:
        if segment.text.strip():  # Ignorer les segments vides
            text_segments.append(segment.text.strip())
    
    text = " ".join(text_segments)
    
    # VÃ©rifier si on a bien capturÃ© du texte
    if not text:
        print("[STT] Aucune parole dÃ©tectÃ©e dans l'audio")
        return {"answer": "Je n'ai pas compris. Pouvez-vous rÃ©pÃ©ter ?", "actions": [], "audio": ""}
    
    print(f"[STT {info.language}] {text}")

    # 3) LLM
    llm_start = time.time()
    answer_text, actions_list = llama(text)
    llm_duration = time.time() - llm_start
    print(f"[LLM TIME] {llm_duration:.2f}s")
    print(f"[LLM] Answer: {answer_text}, Actions: {actions_list}")

    # 4) TTS
    tts_start = time.time()
    mp3_bytes = synthesize(answer_text)
    tts_duration = time.time() - tts_start
    print(f"[TTS TIME] {tts_duration:.2f}s")
    return {"answer": answer_text, "actions": actions_list, "audio": mp3_bytes.hex()}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Endpoint de test  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.get("/health")
async def health_check():
    """Endpoint pour vÃ©rifier que le serveur fonctionne"""
    return {
        "status": "ok",
        "whisper_model": "small",
        "llm_model": "mars-ia-llama3-8B-instruct-q4",
        "tts_voice": "fr_FR-siwis-medium"
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Lancement direct  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import uvicorn
    
    print("=" * 60)
    print("ğŸš€ DÃ©marrage du serveur AI Robot")
    print("=" * 60)
    print(f"ğŸ“ Endpoint: http://0.0.0.0:8000/ask")
    print(f"ğŸ” Token requis: {'Oui' if API_TOKEN else 'Non'}")
    print(f"ğŸ™ï¸  ModÃ¨le STT: Whisper base")
    print(f"ğŸ¤– ModÃ¨le LLM: {LLM}")
    print(f"ğŸ”Š Voix TTS: fr_FR-siwis-medium")
    print("=" * 60)

    uvicorn.run(app, host="0.0.0.0", port=8000)
